# è¯„ä¼°ç®€å• LLM åº”ç”¨

æœ¬æŒ‡å—æ—¨åœ¨è¯´æ˜ä½¿ç”¨ `ragas` æµ‹è¯•å’Œè¯„ä¼° LLM åº”ç”¨çš„ç®€å•å·¥ä½œæµã€‚å‡å®šä½ å¯¹ AI åº”ç”¨æ„å»ºä¸è¯„ä¼°ä»…æœ‰åŸºç¡€äº†è§£ã€‚å®‰è£… `ragas` è¯·å‚è€ƒ [å®‰è£…è¯´æ˜](./install.md)ã€‚

!!! tip "è·å–å¯è¿è¡Œç¤ºä¾‹"
    æœ€å¿«çš„æ–¹å¼æ˜¯ç”¨ quickstart å‘½ä»¤åˆ›å»ºé¡¹ç›®å¹¶æŸ¥çœ‹è¿™äº›æ¦‚å¿µçš„å®é™…æ•ˆæœï¼š

    === "uvxï¼ˆæ¨èï¼‰"
        ```sh
        uvx ragas quickstart rag_eval
        cd rag_eval
        uv sync
        ```

    === "å…ˆå®‰è£… Ragas"
        ```sh
        pip install ragas
        ragas quickstart rag_eval
        cd rag_eval
        uv sync
        ```

    è¿™ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å«ç¤ºä¾‹ä»£ç çš„å®Œæ•´é¡¹ç›®ã€‚å¯ç»“åˆæœ¬æŒ‡å—ç†è§£ç”Ÿæˆä»£ç åœ¨åšä»€ä¹ˆã€‚å¼€å§‹å§ï¼

## é¡¹ç›®ç»“æ„

ç”Ÿæˆçš„é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š

```sh
rag_eval/
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜ä¸é…ç½®æ­¥éª¤
â”œâ”€â”€ pyproject.toml        # uv ä¸ pip çš„é¡¹ç›®é…ç½®
â”œâ”€â”€ evals.py              # ä½ çš„è¯„ä¼°å·¥ä½œæµ
â”œâ”€â”€ rag.py                # ä½ çš„ RAG/LLM åº”ç”¨
â”œâ”€â”€ __init__.py           # ä½¿æœ¬é¡¹ç›®æˆä¸º Python åŒ…
â””â”€â”€ evals/                # è¯„ä¼°äº§ç‰©
    â”œâ”€â”€ datasets/         # æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ experiments/      # è¿è¡Œè¯„ä¼°çš„ç»“æœï¼ˆCSV ä¿å­˜åœ¨æ­¤ï¼‰
    â””â”€â”€ logs/             # è¯„ä¼°æ‰§è¡Œæ—¥å¿—
```

**é‡ç‚¹æ–‡ä»¶ï¼š**

- **`evals.py`** - åŒ…å«æ•°æ®é›†åŠ è½½ä¸è¯„ä¼°é€»è¾‘çš„è¯„ä¼°å·¥ä½œæµ
- **`rag.py`** - ä½ çš„ RAG/LLM åº”ç”¨ä»£ç ï¼ˆæŸ¥è¯¢å¼•æ“ã€æ£€ç´¢ç­‰ï¼‰

## ç†è§£ä»£ç 

åœ¨ç”Ÿæˆé¡¹ç›®çš„ `evals.py` ä¸­ï¼Œä½ ä¼šçœ‹åˆ°ä¸»è¦å·¥ä½œæµæ¨¡å¼ï¼š

1. **åŠ è½½æ•°æ®é›†** - ä½¿ç”¨ `SingleTurnSample` å®šä¹‰æµ‹è¯•ç”¨ä¾‹
2. **æŸ¥è¯¢ RAG ç³»ç»Ÿ** - ä»ä½ çš„åº”ç”¨è·å–å“åº”
3. **è¯„ä¼°å“åº”** - æ ¹æ®å‚è€ƒç­”æ¡ˆéªŒè¯å“åº”
4. **å±•ç¤ºç»“æœ** - åœ¨æ§åˆ¶å°æ˜¾ç¤ºè¯„ä¼°æ‘˜è¦
5. **ä¿å­˜ç»“æœ** - è‡ªåŠ¨ä¿å­˜ä¸º CSV åˆ° `evals/experiments/` ç›®å½•

æ¨¡æ¿æä¾›å¯è‡ªå®šä¹‰çš„æ¨¡å—åŒ–å‡½æ•°ï¼š

```python
from ragas.dataset_schema import SingleTurnSample
from ragas import EvaluationDataset

def load_dataset():
    """åŠ è½½ç”¨äºè¯„ä¼°çš„æµ‹è¯•æ•°æ®é›†ã€‚"""
    data_samples = [
        SingleTurnSample(
            user_input="What is Ragas?",
            response="",  # å°†ç”± RAG æŸ¥è¯¢å¡«å……
            reference="Ragas is an evaluation framework for LLM applications",
            retrieved_contexts=[],
        ),
        # æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹...
    ]
    return EvaluationDataset(samples=data_samples)
```

ä½ å¯ä»¥ç”¨ [æŒ‡æ ‡](../concepts/metrics/available_metrics/index.md) å’Œæ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘è¿›è¡Œæ‰©å±•ã€‚æ›´å¤šå†…å®¹è§ [Ragas ä¸­çš„è¯„ä¼°](../concepts/evaluation/index.md)ã€‚

### é€‰æ‹© LLM æä¾›å•†

å¿«é€Ÿå¼€å§‹é¡¹ç›®é»˜è®¤åœ¨ `_init_clients()` ä¸­åˆå§‹åŒ– OpenAI LLMã€‚ä½ å¯ä»¥é€šè¿‡ `llm_factory` è½»æ¾åˆ‡æ¢åˆ°ä»»æ„æä¾›å•†ï¼š

=== "OpenAI"
    è®¾ç½® OpenAI API å¯†é’¥ï¼š

    ```sh
    export OPENAI_API_KEY="your-openai-key"
    ```

    åœ¨ `evals.py` çš„ `_init_clients()` ä¸­ï¼š

    ```python
    from openai import OpenAI
    from ragas.llms import llm_factory

    client = OpenAI()
    llm = llm_factory("gpt-4o", client=client)
    ```

    å¿«é€Ÿå¼€å§‹é¡¹ç›®ä¸­å·²é…ç½®å¥½ï¼

=== "Anthropic Claude"
    è®¾ç½® Anthropic API å¯†é’¥ï¼š

    ```sh
    export ANTHROPIC_API_KEY="your-anthropic-key"
    ```

    åœ¨ `evals.py` çš„ `_init_clients()` ä¸­ï¼š

    ```python
    import os
    from anthropic import Anthropic
    from ragas.llms import llm_factory

    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
    llm = llm_factory("claude-3-5-sonnet-20241022", provider="anthropic", client=client)
    ```

=== "Google Gemini"
    é…ç½® Google å‡­è¯ï¼š

    ```sh
    export GOOGLE_API_KEY="your-google-api-key"
    ```

    åœ¨ `evals.py` çš„ `_init_clients()` ä¸­ï¼š

    ```python
    import os
    import google.generativeai as genai
    from ragas.llms import llm_factory

    genai.configure(api_key=os.environ.get("GOOGLE_API_KEY"))
    client = genai.GenerativeModel("gemini-2.0-flash")
    llm = llm_factory("gemini-2.0-flash", provider="google", client=client)
    ```

=== "æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰"
    åœ¨æœ¬åœ°å®‰è£…å¹¶è¿è¡Œ Ollamaï¼Œç„¶ååœ¨ `evals.py` çš„ `_init_clients()` ä¸­ï¼š

    ```python
    from openai import OpenAI
    from ragas.llms import llm_factory

    client = OpenAI(
        api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®å¯†é’¥
        base_url="http://localhost:11434/v1"
    )
    llm = llm_factory("mistral", provider="openai", client=client)
    ```

=== "è‡ªå®šä¹‰ / å…¶ä»–æä¾›å•†"
    é€‚ç”¨äºä»»ä½•æä¾› OpenAI å…¼å®¹ API çš„ LLMï¼š

    ```python
    from openai import OpenAI
    from ragas.llms import llm_factory

    client = OpenAI(
        api_key="your-api-key",
        base_url="https://your-api-endpoint"
    )
    llm = llm_factory("model-name", provider="openai", client=client)
    ```

    æ›´å¤šç»†èŠ‚è¯·å‚é˜… [LLM é›†æˆ](../concepts/metrics/index.md)ã€‚

### ä½¿ç”¨é¢„ç½®æŒ‡æ ‡

`ragas` æä¾›å¸¸è§è¯„ä¼°ä»»åŠ¡çš„é¢„ç½®æŒ‡æ ‡ã€‚ä¾‹å¦‚ [Aspect Critique](../concepts/metrics/available_metrics/aspect_critic.md) ä½¿ç”¨ `DiscreteMetric` è¯„ä¼°è¾“å‡ºçš„ä»»æ„ç»´åº¦ï¼š

```python
import asyncio
from openai import AsyncOpenAI
from ragas.metrics import DiscreteMetric
from ragas.llms import llm_factory

# é…ç½®è¯„ä¼°ç”¨ LLM
client = AsyncOpenAI()
evaluator_llm = llm_factory("gpt-4o", client=client)

# åˆ›å»ºè‡ªå®šä¹‰ç»´åº¦è¯„ä¼°å™¨
metric = DiscreteMetric(
    name="summary_accuracy",
    allowed_values=["accurate", "inaccurate"],
    prompt="""Evaluate if the summary is accurate and captures key information.

Response: {response}

Answer with only 'accurate' or 'inaccurate'."""
)

# å¯¹ä½ çš„åº”ç”¨è¾“å‡ºæ‰“åˆ†
async def main():
    score = await metric.ascore(
        llm=evaluator_llm,
        response="The summary of the text is..."
    )
    print(f"Score: {score.value}")  # 'accurate' æˆ– 'inaccurate'
    print(f"Reason: {score.reason}")


if __name__ == "__main__":
    asyncio.run(main())
```

æ­¤ç±»é¢„ç½®æŒ‡æ ‡å¯é¿å…ä»é›¶ç¼–å†™è¯„ä¼°é€»è¾‘ã€‚å¯æµè§ˆ [æ‰€æœ‰å¯ç”¨æŒ‡æ ‡](../concepts/metrics/available_metrics/index.md)ã€‚

!!! info
    `ragas` ä¸­è¿˜æœ‰è®¸å¤šå…¶ä»–ç±»å‹çš„æŒ‡æ ‡ï¼ˆå¸¦æˆ–ä¸å¸¦ `reference`ï¼‰ï¼Œè‹¥éƒ½ä¸æ»¡è¶³éœ€æ±‚ä¹Ÿå¯ä»¥è‡ªå®šä¹‰ã€‚æ›´å¤šå†…å®¹è¯·æŸ¥çœ‹ [æŒ‡æ ‡è¯¦è§£](../concepts/metrics/index.md)ã€‚

### åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°

åœ¨å¿«é€Ÿå¼€å§‹é¡¹ç›®ä¸­ï¼Œ`load_dataset()` ä¼šåˆ›å»ºåŒ…å«å¤šä¸ªæ ·æœ¬çš„æµ‹è¯•æ•°æ®ï¼š

```python
from ragas import Dataset

# åˆ›å»ºåŒ…å«å¤šä¸ªæµ‹è¯•æ ·æœ¬çš„æ•°æ®é›†
dataset = Dataset(
    name="test_dataset",
    backend="local/csv",  # ä¹Ÿå¯ä½¿ç”¨ JSONLã€Google Drive æˆ– in-memory
    root_dir=".",
)

# å‘æ•°æ®é›†æ·»åŠ æ ·æœ¬
data_samples = [
    {
        "user_input": "What is ragas?",
        "response": "Ragas is an evaluation framework...",
        "expected": "Ragas provides objective metrics..."
    },
    {
        "user_input": "How do metrics work?",
        "response": "Metrics score your application...",
        "expected": "Metrics evaluate performance..."
    },
]

for sample in data_samples:
    dataset.append(sample)

# ä¿å­˜åˆ°ç£ç›˜
dataset.save()
```

è¿™æ ·å°±æœ‰å¤šä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡åªè¯„ä¼°ä¸€ä¸ªæ ·æœ¬ã€‚æ›´å¤šå†…å®¹è§ [æ•°æ®é›†ä¸å®éªŒ](../concepts/components/eval_dataset.md)ã€‚

ç”Ÿæˆçš„é¡¹ç›®åœ¨ `evals/datasets/` ä¸‹åŒ…å«ç¤ºä¾‹æ•°æ®ï¼Œå¯ç¼–è¾‘è¿™äº›æ–‡ä»¶ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹ã€‚

### éœ€è¦å€ŸåŠ©è¯„ä¼°æ”¹è¿› AI åº”ç”¨ï¼Ÿ

è¿‡å»ä¸¤å¹´é‡Œï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°å¸®åŠ©äº†è®¸å¤š AI åº”ç”¨æ”¹è¿›ã€‚

æˆ‘ä»¬æ­£åœ¨æŠŠè¿™äº›ç»éªŒæ²‰æ·€æˆäº§å“ï¼Œç”¨è¯„ä¼°å¾ªç¯æ›¿ä»£â€œæ„Ÿè§‰å¥½ä¸å¥½â€çš„æ£€æŸ¥ï¼Œè®©ä½ æ›´ä¸“æ³¨äºæŠŠ AI åº”ç”¨åšå¥½ã€‚

è‹¥ä½ å¸Œæœ›å€ŸåŠ©è¯„ä¼°æ”¹è¿›å’Œæ‰©å±• AI åº”ç”¨ï¼š

ğŸ”— é¢„çº¦ [æ—¶æ®µ](https://bit.ly/3EBYq4J) æˆ–å‘é‚®ä»¶ï¼š[founders@vibrantlabs.com](mailto:founders@vibrantlabs.com)ã€‚

![](../_static/ragas_app.gif)

## ä¸‹ä¸€æ­¥

- [è¯„ä¼°ç®€å• RAG åº”ç”¨](rag_eval.md)
