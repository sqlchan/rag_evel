# å¿«é€Ÿå¼€å§‹ï¼šå‡ åˆ†é’Ÿå†…è¿è¡Œè¯„ä¼°

å‡ åˆ†é’Ÿå†…ä¸Šæ‰‹ Ragasï¼Œç”¨å°‘é‡å‘½ä»¤å³å¯åˆ›å»ºå®Œæ•´çš„è¯„ä¼°é¡¹ç›®ã€‚

## æ­¥éª¤ 1ï¼šåˆ›å»ºé¡¹ç›®

ä»»é€‰ä¸€ç§æ–¹å¼ï¼š

=== "uvxï¼ˆæ¨èï¼‰"
    æ— éœ€å®‰è£…ã€‚`uvx` ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶è¿è¡Œ ragasï¼š

    ```sh
    uvx ragas quickstart rag_eval
    cd rag_eval
    ```

=== "å…ˆå®‰è£… Ragas"
    å…ˆå®‰è£… ragasï¼Œå†åˆ›å»ºé¡¹ç›®ï¼š

    ```sh
    pip install ragas
    ragas quickstart rag_eval
    cd rag_eval
    ```

## æ­¥éª¤ 2ï¼šå®‰è£…ä¾èµ–

å®‰è£…é¡¹ç›®ä¾èµ–ï¼š

```sh
uv sync
```

è‹¥åå¥½ `pip`ï¼š

```sh
pip install -e .
```

## æ­¥éª¤ 3ï¼šè®¾ç½® API å¯†é’¥

é»˜è®¤æƒ…å†µä¸‹ï¼Œå¿«é€Ÿå¼€å§‹ç¤ºä¾‹ä½¿ç”¨ OpenAIã€‚è®¾ç½®å¥½ API å¯†é’¥å³å¯å¼€å§‹ã€‚ä¹Ÿå¯é€šè¿‡å°å¹…ä¿®æ”¹ä½¿ç”¨å…¶ä»–æä¾›å•†ï¼š

=== "OpenAIï¼ˆé»˜è®¤ï¼‰"
    ```sh
    export OPENAI_API_KEY="your-openai-key"
    ```

    å¿«é€Ÿå¼€å§‹é¡¹ç›®å·²é…ç½®ä¸ºä½¿ç”¨ OpenAIï¼Œæ— éœ€é¢å¤–è®¾ç½®ã€‚

=== "Anthropic Claude"
    è®¾ç½® Anthropic API å¯†é’¥ï¼š

    ```sh
    export ANTHROPIC_API_KEY="your-anthropic-key"
    ```

    ç„¶ååœ¨ `evals.py` ä¸­æ›´æ–° LLM åˆå§‹åŒ–ï¼š

    ```python
    from anthropic import Anthropic
    from ragas.llms import llm_factory

    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
    llm = llm_factory("claude-3-5-sonnet-20241022", provider="anthropic", client=client)
    ```

=== "Google Gemini"
    é…ç½® Google å‡­è¯ï¼š

    ```sh
    export GOOGLE_API_KEY="your-google-api-key"
    ```

    ç„¶ååœ¨ `evals.py` ä¸­æ›´æ–° LLM åˆå§‹åŒ–ï¼š

    **æ–¹å¼ä¸€ï¼šä½¿ç”¨ Google å®˜æ–¹åº“ï¼ˆæ¨èï¼‰**

    ```python
    import google.generativeai as genai
    from ragas.llms import llm_factory

    genai.configure(api_key=os.environ.get("GOOGLE_API_KEY"))
    client = genai.GenerativeModel("gemini-2.0-flash")
    llm = llm_factory("gemini-2.0-flash", provider="google", client=client)
    # é€‚é…å™¨ä¼šä¸º google æä¾›å•†è‡ªåŠ¨è¯†åˆ«ä¸º "litellm"
    ```

    æ›´å¤š Gemini é€‰é¡¹ä¸è¯¦ç»†é…ç½®è¯·å‚é˜… [Google Gemini é›†æˆæŒ‡å—](../howtos/integrations/gemini.md)ã€‚

=== "æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰"
    åœ¨æœ¬åœ°å®‰è£…å¹¶è¿è¡Œ Ollamaï¼Œç„¶ååœ¨ `evals.py` ä¸­æ›´æ–° LLM åˆå§‹åŒ–ï¼š

    ```python
    from openai import OpenAI
    from ragas.llms import llm_factory

    # ä¸º Ollama åˆ›å»º OpenAI å…¼å®¹å®¢æˆ·ç«¯
    client = OpenAI(
        api_key="ollama",  # Ollama ä¸éœ€è¦çœŸå®å¯†é’¥
        base_url="http://localhost:11434/v1"
    )
    llm = llm_factory("mistral", provider="openai", client=client)
    ```

=== "è‡ªå®šä¹‰ / å…¶ä»–æä¾›å•†"
    é€‚ç”¨äºä»»ä½•æä¾› OpenAI å…¼å®¹ API çš„ LLMï¼š

    ```python
    from openai import OpenAI
    from ragas.llms import llm_factory

    client = OpenAI(
        api_key="your-api-key",
        base_url="https://your-api-endpoint"
    )
    llm = llm_factory("model-name", provider="openai", client=client)
    ```

    æ›´å¤šç»†èŠ‚è¯·å‚é˜… [LLM é›†æˆ](../concepts/metrics/index.md)ã€‚

## é¡¹ç›®ç»“æ„

ç”Ÿæˆçš„é¡¹ç›®åŒ…å«ï¼š

```sh
rag_eval/
â”œâ”€â”€ README.md              # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ pyproject.toml         # é¡¹ç›®é…ç½®
â”œâ”€â”€ rag.py                 # ä½ çš„ RAG åº”ç”¨
â”œâ”€â”€ evals.py               # è¯„ä¼°å·¥ä½œæµ
â”œâ”€â”€ __init__.py            # ä½¿æœ¬é¡¹ç›®æˆä¸º Python åŒ…
â””â”€â”€ evals/
    â”œâ”€â”€ datasets/          # æµ‹è¯•æ•°æ®æ–‡ä»¶
    â”œâ”€â”€ experiments/       # è¯„ä¼°ç»“æœ
    â””â”€â”€ logs/              # æ‰§è¡Œæ—¥å¿—
```

## æ­¥éª¤ 4ï¼šè¿è¡Œè¯„ä¼°

è¿è¡Œè¯„ä¼°è„šæœ¬ï¼š

```sh
uv run python evals.py
```

è‹¥ä½¿ç”¨ `pip` å®‰è£…ï¼š

```sh
python evals.py
```

è¯„ä¼°å°†ï¼š

- ä» `evals.py` ä¸­çš„ `load_dataset()` åŠ è½½æµ‹è¯•æ•°æ®
- ç”¨æµ‹è¯•é—®é¢˜è°ƒç”¨ä½ çš„ RAG åº”ç”¨
- å¯¹å“åº”è¿›è¡Œè¯„ä¼°
- åœ¨æ§åˆ¶å°å±•ç¤ºç»“æœ
- å°†ç»“æœä¿å­˜ä¸º CSV åˆ° `evals/experiments/` ç›®å½•

![](../_static/imgs/results/rag_eval_result.png)

æ­å–œï¼Œä½ å·²ç»æœ‰ä¸€å¥—å®Œæ•´çš„è¯„ä¼°ç¯å¢ƒåœ¨è¿è¡Œã€‚ğŸ‰

---

## è‡ªå®šä¹‰è¯„ä¼°

### æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹

åœ¨ `evals.py` ä¸­ç¼–è¾‘ `load_dataset()`ï¼Œå¢åŠ æ›´å¤šæµ‹è¯•é—®é¢˜ï¼š

```python
from ragas import Dataset

def load_dataset():
    """åŠ è½½ç”¨äºè¯„ä¼°çš„æµ‹è¯•æ•°æ®é›†ã€‚"""
    dataset = Dataset(
        name="test_dataset",
        backend="local/csv",
        root_dir=".",
    )

    data_samples = [
        {
            "question": "What is Ragas?",
            "grading_notes": "Ragas is an evaluation framework for LLM applications",
        },
        {
            "question": "How do metrics work?",
            "grading_notes": "Metrics evaluate the quality and performance of LLM responses",
        },
        # åœ¨æ­¤æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹
    ]

    for sample in data_samples:
        dataset.append(sample)

    dataset.save()
    return dataset
```

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ¨¡æ¿ä¸­åŒ…å«ç”¨äºè‡ªå®šä¹‰è¯„ä¼°é€»è¾‘çš„ `DiscreteMetric`ã€‚å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è‡ªå®šä¹‰è¯„ä¼°ï¼š

1. **ä¿®æ”¹æŒ‡æ ‡æç¤º** - è°ƒæ•´è¯„ä¼°æ ‡å‡†
2. **è°ƒæ•´å…è®¸å€¼** - æ›´æ–°æœ‰æ•ˆè¾“å‡ºç±»åˆ«
3. **æ·»åŠ æ›´å¤šæŒ‡æ ‡** - ä¸ºä¸åŒç»´åº¦åˆ›å»ºé¢å¤–æŒ‡æ ‡

ä¿®æ”¹æŒ‡æ ‡ç¤ºä¾‹ï¼š

```python
from ragas.metrics import DiscreteMetric
from ragas.llms import llm_factory

my_metric = DiscreteMetric(
    name="custom_evaluation",
    prompt="Evaluate this response: {response} based on: {context}. Return 'excellent', 'good', or 'poor'.",
    allowed_values=["excellent", "good", "poor"],
)
```

## ä¸‹ä¸€æ­¥

- **ç†è§£æ¦‚å¿µ**ï¼šé˜…è¯» [è¯„ä¼°ç®€å• LLM åº”ç”¨](evals.md) ä»¥æ·±å…¥ç†è§£
- **è‡ªå®šä¹‰æŒ‡æ ‡**ï¼šä½¿ç”¨ç®€å•è£…é¥°å™¨ [åˆ›å»ºè‡ªå·±çš„æŒ‡æ ‡](../concepts/metrics/overview/index.md#output-types)
- **ç”Ÿäº§é›†æˆ**ï¼šå°†è¯„ä¼° [é›†æˆåˆ° CI/CD æµæ°´çº¿](../howtos/index.md)
- **RAG è¯„ä¼°**ï¼šä½¿ç”¨ä¸“ç”¨æŒ‡æ ‡ [è¯„ä¼° RAG ç³»ç»Ÿ](rag_eval.md)
- **Agent è¯„ä¼°**ï¼šäº†è§£ [AI Agent è¯„ä¼°](../howtos/applications/text2sql.md)
- **æµ‹è¯•æ•°æ®ç”Ÿæˆ**ï¼šä¸ºè¯„ä¼° [ç”Ÿæˆåˆæˆæµ‹è¯•æ•°æ®é›†](rag_testset_generation.md)

## è·å–å¸®åŠ©

- ğŸ“š [å®Œæ•´æ–‡æ¡£](https://docs.ragas.io/)
- ğŸ’¬ [åŠ å…¥ Discord ç¤¾åŒº](https://discord.gg/5djav8GGNZ)
- ğŸ› [åé¦ˆé—®é¢˜](https://github.com/vibrantlabsai/ragas/issues)
