# å¦‚ä½•å°† LLM å¯¹é½ä¸ºè¯„åˆ¤ï¼ˆJudgeï¼‰

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Ragas ç³»ç»Ÿæ€§åœ°è¯„ä¼°å¹¶å°†â€œLLM å³è¯„åˆ¤â€æŒ‡æ ‡ä¸äººç±»ä¸“å®¶åˆ¤æ–­å¯¹é½ã€‚

- æ­å»ºå¯å¤ç”¨çš„è¯„åˆ¤å¯¹é½è¯„ä¼°æµæ°´çº¿
- åˆ†æè¯„åˆ¤ä¸äººå·¥æ ‡ç­¾ä¹‹é—´çš„ä¸ä¸€è‡´æ¨¡å¼
- è¿­ä»£è¯„åˆ¤æç¤ºè¯ä»¥æå‡ä¸ä¸“å®¶å†³ç­–çš„å¯¹é½åº¦

## ä¸ºä½•è¦å…ˆå¯¹é½ LLM è¯„åˆ¤ï¼Ÿ

åœ¨è·‘è¯„ä¼°å®éªŒä¹‹å‰ï¼Œå…ˆæŠŠ LLM è¯„åˆ¤å¯¹é½åˆ°ä½ çš„å…·ä½“åœºæ™¯å¾ˆé‡è¦ã€‚æœªå¯¹é½çš„è¯„åˆ¤å°±åƒæŒ‡é”™æ–¹å‘çš„æŒ‡å—é’ˆâ€”â€”åŸºäºå®ƒçš„æŒ‡å¯¼æ‰€åšçš„â€œæ”¹è¿›â€ä¼šç¦»ç›®æ ‡æ›´è¿œã€‚è®©è¯„åˆ¤ä¸ä¸“å®¶åˆ¤æ–­ä¸€è‡´ï¼Œæ‰èƒ½æ”¹è¿›çœŸæ­£é‡è¦çš„ç»´åº¦ã€‚å¯¹é½æ­¥éª¤æ˜¯å¯é è¯„ä¼°çš„åŸºç¡€ã€‚

!!! tip "çœŸæ­£ä»·å€¼ï¼šçœ‹ä½ çš„æ•°æ®"
    æ„å»ºä¸€ä¸ªå¯¹é½çš„ LLM è¯„åˆ¤æœ‰ç”¨ï¼Œä½†çœŸæ­£çš„ä¸šåŠ¡ä»·å€¼æ¥è‡ªç³»ç»Ÿæ€§åœ°åˆ†ææ•°æ®ã€ç†è§£å¤±è´¥æ¨¡å¼ã€‚è¯„åˆ¤å¯¹é½è¿‡ç¨‹ä¼šè¿«ä½¿ä½ æ·±å…¥å®¡è§†è¾¹ç•Œæ¡ˆä¾‹ã€æ¾„æ¸…è¯„ä¼°æ ‡å‡†ï¼Œå¹¶å‘ç°â€œä»€ä¹ˆç®—å¥½/åå›ç­”â€çš„æ´å¯Ÿã€‚æŠŠè¯„åˆ¤å½“ä½œæ”¾å¤§åˆ†æèƒ½åŠ›çš„å·¥å…·ï¼Œè€Œä¸æ˜¯æ›¿ä»£åˆ†ææœ¬èº«ã€‚

## é…ç½®ç¯å¢ƒ

æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªå¯å®‰è£…è¿è¡Œçš„ç®€å•æ¨¡å—ï¼Œæ–¹ä¾¿ä½ ä¸“æ³¨äºç†è§£è¯„ä¼°æµç¨‹è€Œéä»é›¶å†™åº”ç”¨ã€‚

```bash
uv pip install "ragas[examples]"
export OPENAI_API_KEY="your-api-key-here"
```

!!! note "å®Œæ•´ä»£ç "
    è¯„åˆ¤å¯¹é½è¯„ä¼°æµæ°´çº¿çš„å®Œæ•´ä»£ç è§[æ­¤å¤„](https://github.com/vibrantlabsai/ragas/tree/main/examples/ragas_examples/judge_alignment)ã€‚

## ç†è§£æ•°æ®é›†

æˆ‘ä»¬ä½¿ç”¨ [EvalsBench æ•°æ®é›†](https://github.com/vibrantlabsai/EvalsBench/blob/main/data/benchmark_df.csv)ï¼Œå…¶ä¸­åŒ…å«é’ˆå¯¹å•†ä¸šé—®é¢˜çš„ LLM å›ç­”åŠä¸“å®¶æ ‡æ³¨ã€‚æ¯è¡ŒåŒ…æ‹¬ï¼š

- `question`ï¼šåŸå§‹é—®é¢˜
- `grading_notes`ï¼šå¥½å›ç­”åº”æ¶µç›–çš„è¦ç‚¹
- `response`ï¼šLLM ç”Ÿæˆçš„å›ç­”
- `target`ï¼šäººç±»ä¸“å®¶çš„äºŒå€¼åˆ¤æ–­ï¼ˆpass/failï¼‰

**ä¸‹è½½æ•°æ®é›†ï¼š**

```bash
# Create datasets folder and download the dataset
mkdir -p datasets
curl -o datasets/benchmark_df.csv https://raw.githubusercontent.com/vibrantlabsai/EvalsBench/main/data/benchmark_df.csv
```

**åŠ è½½å¹¶æŸ¥çœ‹æ•°æ®é›†ï¼š**

```python
import pandas as pd
from ragas import Dataset

def load_dataset(csv_path: str = None) -> Dataset:
    """Load annotated dataset with human judgments.
    
    Expected columns: question, grading_notes, response, target (pass/fail)
    """
    path = csv_path or "datasets/benchmark_df.csv"
    df = pd.read_csv(path)

    dataset = Dataset(name="llm_judge_alignment", backend="local/csv")
    
    for _, row in df.iterrows():
        dataset.append({
            "question": row["question"],
            "grading_notes": row["grading_notes"],
            "response": row["response"],
            "target": (row["target"]),
        })
    
    return dataset

# Load the dataset
dataset = load_dataset()
print(f"Dataset loaded with {len(dataset)} samples")
```

**æ•°æ®é›†ç¤ºä¾‹è¡Œï¼š**

| question | grading_notes | response | target |
|----------|---------------|----------|---------|
| What are the key methods for determining the pre-money valuation of a tech startup before a Series A investment round, and how do they differ? | DCF method: !future cash flows!, requires projections; Comp. analysis: similar co. multiples; VC method: rev x multiple - post-$; *Founder's share matter*; strategic buyers pay more. | Determining the pre-money valuation of a tech startup before a Series A investment round is a critical step... (covers DCF, comparable analysis, VC method) | pass |
| What key metrics and strategies should a startup prioritize to effectively manage and reduce churn rate in a subscription-based business model? | Churn:! monitor monthly, <5% ideal. *Retention strategies*: engage users, improve onboarding. CAC & LTV: balance 3:1+. Feedback loops: implement early. *Customer support*: proactive & responsive, critical. | Managing and reducing churn rate in a subscription-based business model is crucial... (missing specific metrics and strategies) | fail |

æ•°æ®é›†ä¸­åŒä¸€é—®é¢˜æœ‰å¤šæ¡å›ç­”â€”â€”æœ‰çš„ passã€æœ‰çš„ failï¼Œæœ‰åŠ©äºè¯„åˆ¤å­¦ä¹ â€œå¯æ¥å—â€ä¸â€œä¸å¯æ¥å—â€çš„ç»†å¾®å·®åˆ«ã€‚

!!! info "ç†è§£ä½ çš„æ ‡å‡†ç­”æ¡ˆ"
    è¯„åˆ¤å¯¹é½çš„è´¨é‡å®Œå…¨å–å†³äºæ ‡å‡†ç­”æ¡ˆï¼ˆground truthï¼‰æ ‡ç­¾çš„è´¨é‡ã€‚åœ¨ç”Ÿäº§åœºæ™¯ä¸­ï¼Œåº”è®©**é¢†åŸŸä¸»ä¸“å®¶**å‚ä¸â€”â€”å³å¯¹ä½ åœºæ™¯æœ€é‡è¦çš„é‚£ä¸ªäººï¼ˆå¦‚å¿ƒç†å¥åº· AI ç”¨å¿ƒç†å­¦å®¶ã€æ³•å¾‹ AI ç”¨å¾‹å¸ˆã€å®¢æœèŠå¤©æœºå™¨äººç”¨å®¢æœæ€»ç›‘ï¼‰ã€‚ä»–ä»¬ç¨³å®šä¸€è‡´çš„åˆ¤æ–­åº”æˆä¸ºè¯„åˆ¤å¯¹é½çš„é‡‘æ ‡å‡†ã€‚ä¸éœ€è¦ç»™æ¯æ¡æ ·æœ¬éƒ½æ‰“æ ‡ï¼Œæœ‰ä»£è¡¨æ€§çš„ 100â€“200 æ¡è¦†ç›–å¤šç§åœºæ™¯å³å¯è·å¾—å¯é å¯¹é½ã€‚

## ç†è§£è¯„ä¼°æ–¹å¼

æœ¬æŒ‡å—ä¸­å¯¹æ•°æ®é›†ä¸­å·²æœ‰çš„å›ç­”åšè¯„ä¼°ï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆã€‚è¿™æ ·ä¿è¯å¤šæ¬¡è¿è¡Œå¯å¤ç°ï¼Œå¹¶è®©æˆ‘ä»¬ä¸“æ³¨äºè¯„åˆ¤å¯¹é½è€Œéç”Ÿæˆã€‚

è¯„ä¼°æµç¨‹ï¼š**æ•°æ®è¡Œï¼ˆé—®é¢˜ + å›ç­”ï¼‰â†’ è¯„åˆ¤ â†’ ä¸äººç±» target å¯¹æ¯”**

## å®šä¹‰è¯„ä¼°æŒ‡æ ‡

è¯„åˆ¤å¯¹é½éœ€è¦ä¸¤ä¸ªæŒ‡æ ‡ï¼š

**ä¸»æŒ‡æ ‡ï¼š`accuracy`ï¼ˆLLM è¯„åˆ¤ï¼‰** â€” å¯¹å›ç­”æ‰“åˆ†å¹¶è¿”å› pass/fail åŠç†ç”±ã€‚

**å¯¹é½æŒ‡æ ‡ï¼š`judge_alignment`** â€” æ£€æŸ¥è¯„åˆ¤ç»“è®ºæ˜¯å¦ä¸äººç±»ä¸“å®¶ä¸€è‡´ã€‚

### é…ç½®è¯„åˆ¤æŒ‡æ ‡

å®šä¹‰ä¸€ä¸ªæ ¹æ® grading notes è¯„ä¼°å›ç­”çš„ç®€å•åŸºçº¿è¯„åˆ¤æŒ‡æ ‡ï¼š

```python
from ragas.metrics import DiscreteMetric

# Define the judge metric with a simple baseline prompt
accuracy_metric = DiscreteMetric(
    name="accuracy",
    prompt="Check if the response contains points mentioned from the grading notes and return 'pass' or 'fail'.\n\nResponse: {response}\nGrading Notes: {grading_notes}",
    allowed_values=["pass", "fail"],
)
```

### å¯¹é½æŒ‡æ ‡

å¯¹é½æŒ‡æ ‡å°†è¯„åˆ¤ç»“è®ºä¸äººç±»ç»“è®ºæ¯”è¾ƒï¼š

```python
from ragas.metrics.discrete import discrete_metric
from ragas.metrics.result import MetricResult

@discrete_metric(name="judge_alignment", allowed_values=["pass", "fail"])
def judge_alignment(judge_label: str, human_label: str) -> MetricResult:
    """Compare judge decision with human label."""
    judge = judge_label.strip().lower()
    human = human_label.strip().lower()
    
    if judge == human:
        return MetricResult(value="pass", reason=f"Judge={judge}; Human={human}")
    
    return MetricResult(value="fail", reason=f"Judge={judge}; Human={human}")
```

## å®éªŒå‡½æ•°

[å®éªŒå‡½æ•°](../../concepts/experimentation_zh.md) è´Ÿè´£å®Œæ•´è¯„ä¼°æµæ°´çº¿ï¼šç”¨è¯„åˆ¤å¯¹å›ç­”æ‰“åˆ†å¹¶æµ‹é‡ä¸äººç±»çš„å¯¹é½åº¦ï¼š

```python
from typing import Dict, Any
from ragas import experiment
from ragas.metrics import DiscreteMetric
from ragas_examples.judge_alignment import judge_alignment  # The metric we created above

@experiment()
async def judge_experiment(
    row: Dict[str, Any],
    accuracy_metric: DiscreteMetric,
    llm,
):
    """Run complete evaluation: Judge â†’ Compare with human."""
    # Step 1: Get response (in production, this is where you'd call your LLM app)
    # For this evaluation, we use pre-existing responses from the dataset
    app_response = row["response"]
    
    # Step 2: Judge evaluates the response
    judge_score = await accuracy_metric.ascore(
        question=row["question"],
        grading_notes=row["grading_notes"],
        response=app_response,
        llm=llm,
    )

    # Step 3: Compare judge decision with human target
    alignment = judge_alignment.score(
        judge_label=judge_score.value,
        human_label=row["target"]
    )

    return {
        **row,
        "judge_label": judge_score.value,
        "judge_reason": judge_score.reason,
        "alignment": alignment.value,
        "alignment_reason": alignment.reason,
    }
```

## è¿è¡ŒåŸºçº¿è¯„ä¼°

### æ‰§è¡Œè¯„ä¼°æµæ°´çº¿å¹¶æ”¶é›†ç»“æœ

```python
import os
from openai import AsyncOpenAI
from ragas.llms import llm_factory
from ragas_examples.judge_alignment import load_dataset

# Load dataset
dataset = load_dataset()
print(f"Dataset loaded with {len(dataset)} samples")

# Initialize LLM client
openai_client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
llm = llm_factory("gpt-4o-mini", client=openai_client)

# Run the experiment
results = await judge_experiment.arun(
    dataset,
    name="judge_baseline_v1_gpt-4o-mini",
    accuracy_metric=accuracy_metric,
    llm=llm,
)

# Calculate alignment rate
passed = sum(1 for r in results if r["alignment"] == "pass")
total = len(results)
print(f"âœ… Baseline alignment: {passed}/{total} passed ({passed/total:.1%})")
```

??? "ğŸ“‹ Output (baseline v1)"

    ```text
    2025-10-08 22:40:00,334 - Loaded dataset with 160 samples
    2025-10-08 22:40:00,334 - Initializing LLM client with model: gpt-4o-mini
    2025-10-08 22:40:01,858 - Running baseline evaluation...
    Running experiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [04:35<00:00,  1.72s/it]
    2025-10-08 22:44:37,149 - âœ… Baseline alignment: 121/160 passed (75.6%)
    ```

### åˆæ­¥è¡¨ç°åˆ†æ

è¯„ä¼°ä¼šç”ŸæˆåŒ…å«æ‰€æœ‰è¾“å…¥ï¼ˆquestionã€grading_notesã€responseï¼‰ã€äººç±» targetã€è¯„åˆ¤ç»“è®ºåŠç†ç”±ä»¥åŠå¯¹é½æ¯”è¾ƒçš„ CSV ç»“æœã€‚

## åˆ†æé”™è¯¯ä¸å¤±è´¥æ¨¡å¼

è·‘å®ŒåŸºçº¿è¯„ä¼°åï¼Œå¯ä»¥åˆ†æä¸ä¸€è‡´æ¨¡å¼ï¼Œçœ‹è¯„åˆ¤åœ¨å“ªäº›åœ°æ–¹ä¸ä¸“å®¶ä¸ä¸€è‡´ã€‚

**åŸºçº¿è¡¨ç°ï¼š75.6% å¯¹é½ï¼ˆ121/160 æ­£ç¡®ï¼‰**

æŸ¥çœ‹é”™è¯¯åˆ†å¸ƒï¼š

??? admonition "ğŸ“‹ Code"

    ```python
    import pandas as pd

    # Load results
    df = pd.read_csv('experiments/judge_baseline_v1_gpt-4o-mini.csv')

    # Analyze misalignments
    false_positives = len(df[(df['judge_label'] == 'pass') & (df['target'] == 'fail')])
    false_negatives = len(df[(df['judge_label'] == 'fail') & (df['target'] == 'pass')])

    print(f"False positives (judge too lenient): {false_positives}")
    print(f"False negatives (judge too strict): {false_negatives}")
    ```

    ğŸ“‹ Output

    ```text
    False positives (judge too lenient): 39
    False negatives (judge too strict): 0
    ```

**ä¸»è¦ç»“è®ºï¼š** å…¨éƒ¨ 39 ä¸ªä¸ä¸€è‡´ï¼ˆ24.4%ï¼‰éƒ½æ˜¯å‡é˜³æ€§â€”â€”è¯„åˆ¤ç»™äº† pass è€Œä¸“å®¶ç»™äº† failã€‚åŸºçº¿è¯„åˆ¤è¿‡äºå®½æ¾ï¼Œæ¼æ‰äº†æœªæ¶µç›– grading notes ä¸­å…³é”®æ¦‚å¿µçš„å›ç­”ã€‚

### å¤±è´¥æ¡ˆä¾‹ç¤ºä¾‹

ä»¥ä¸‹ä¸ºè¯„åˆ¤é”™è¯¯åœ°ç»™äº† passã€ä½†ç¼ºå°‘å…³é”®æ¦‚å¿µçš„å›ç­”ç¤ºä¾‹ï¼š

| Grading Notes | Human Label | Judge Label | What's Missing |
|---------------|-------------|-------------|----------------|
| `*Valuation caps*, $, post-$ val key. Liquidation prefs: 1x+ common. Anti-dilution: *full vs. weighted*. Board seats: 1-2 investor reps. ESOP: 10-20%.` | fail | pass | Response discusses all points comprehensively but human annotators marked it as fail for subtle omissions |
| `*Impact on valuation*: scalability potential, dev costs, integration ease. !Open-source vs proprietary issues. !Tech debt risks. Discuss AWS/GCP/Azure...` | fail | pass | Missing specific discussion of post-money valuation impact |
| `Historical vs. forecasted rev; top-down & bottom-up methods; *traction evidence*; !unbiased assumptions; 12-24mo project...` | fail | pass | Missing explicit mention of traction evidence |

**é”™è¯¯ä¸­çš„å¸¸è§æ¨¡å¼ï¼š**

1. **ç¼ºå°‘ 1â€“2 ä¸ªå…·ä½“æ¦‚å¿µ**ï¼Œå…¶ä»–æ¦‚å¿µéƒ½æœ‰
2. **éšå« vs æ˜¾å¼**ï¼šè¯„åˆ¤æ¥å—äº†éšå«è¡¨è¿°ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›æ˜¾å¼æåˆ°
3. **ç¼©å†™æœªæ­£ç¡®è§£æ**ï¼ˆå¦‚ "mkt demand" = market demandï¼Œ"post-$" = post-money valuationï¼‰
4. **å¿½ç•¥å…³é”®æ ‡è®°**ï¼šå¸¦ `*` æˆ– `!` çš„è¦ç‚¹å¾€å¾€å¿…ä¸å¯å°‘

## æ”¹è¿›è¯„åˆ¤æç¤ºè¯

æ ¹æ®é”™è¯¯åˆ†æï¼Œéœ€è¦æ„é€ ä¸€ä¸ªæ”¹è¿›ç‰ˆæç¤ºè¯ï¼Œè¦æ±‚ï¼š

1. **ç†è§£** grading notes ä¸­çš„ç¼©å†™
2. **è¯†åˆ«å…³é”®æ ‡è®°**ï¼ˆ`*`ã€`!`ã€å…·ä½“æ•°å­—ï¼‰
3. **è¦æ±‚æ‰€æœ‰æ¦‚å¿µ**éƒ½å‡ºç°ï¼Œè€Œä¸æ˜¯å¤§éƒ¨åˆ†
4. **æ¥å—è¯­ä¹‰ç­‰ä»·**ï¼ˆåŒä¸€æ¦‚å¿µçš„ä¸åŒè¡¨è¿°ï¼‰
5. **æ¾ç´§é€‚ä¸­**â€”â€”æ—¢ä¸è¿‡æ¾ä¹Ÿä¸è¿‡ä¸¥

### ç¼–å†™æ”¹è¿›ç‰ˆ v2 æç¤ºè¯

ç”¨æ›´å®Œæ•´çš„è¯„ä¼°æ ‡å‡†å®šä¹‰å¢å¼ºç‰ˆè¯„åˆ¤æŒ‡æ ‡ï¼š

```python
from ragas.metrics import DiscreteMetric

# Define improved judge metric with enhanced evaluation criteria
accuracy_metric_v2 = DiscreteMetric(
    name="accuracy",
    prompt="""Evaluate if the response covers ALL the key concepts from the grading notes. Accept semantic equivalents but carefully check for missing concepts.

ABBREVIATION GUIDE - decode these correctly:

â€¢ Financial: val=valuation, post-$=post-money, rev=revenue, ARR/MRR=Annual/Monthly Recurring Revenue, COGS=Cost of Goods Sold, Opex=Operating Expenses, LTV=Lifetime Value, CAC=Customer Acquisition Cost
â€¢ Business: mkt=market, reg/regs=regulation/regulatory, corp gov=corporate governance, integr=integration, S&M=Sales & Marketing, R&D=Research & Development, acq=acquisition
â€¢ Technical: sys=system, elim=elimination, IP=Intellectual Property, TAM=Total Addressable Market, diff=differentiation
â€¢ Metrics: NPS=Net Promoter Score, SROI=Social Return on Investment, proj=projection, cert=certification

EVALUATION APPROACH:

Step 1 - Parse grading notes into distinct concepts:

- Separate by commas, semicolons, or line breaks
- Each item is a concept that must be verified
- Example: "*Gross Margin* >40%, CAC, LTV:CAC >3:1" = 3 concepts

Step 2 - For each concept, check if it's addressed:

- Accept semantic equivalents (e.g., "customer acquisition cost" = "CAC")
- Accept implicit coverage when it's clear (e.g., "revenue forecasting" covers "historical vs forecasted rev")
- Be flexible on exact numbers (e.g., "around 40%" acceptable for ">40%")

Step 3 - Count missing concepts:

- Missing 0 concepts = PASS
- Missing 1+ concepts = FAIL (even one genuinely missing concept should fail)
- Exception: If a long list (10+ items) has 1 very minor detail missing but all major points covered, use judgment

CRITICAL RULES:

1. Do NOT require exact wording - "market demand" = "mkt demand" = "demand analysis"

2. Markers (* or !) mean important, not mandatory exact phrases:
   - "*traction evidence*" can be satisfied by discussing metrics, growth, or validation
   - "!unbiased assumptions" can be satisfied by discussing assumption methodology

3. Numbers should be mentioned but accept approximations:
   - "$47B to $10B" can be "$47 billion dropped to around $10 billion"
   - "LTV:CAC >3:1" can be "LTV to CAC ratio of at least 3 to 1" or "3x or higher"

4. FAIL only when concepts are genuinely absent:
   - If notes mention "liquidation prefs, anti-dilution, board seats" but response only has board seats â†’ FAIL
   - If notes mention "scalability, tech debt, IP" but response never discusses technical risks â†’ FAIL
   - If notes mention "GDPR compliance" and response never mentions GDPR or EU regulations â†’ FAIL

5. PASS when ALL concepts present:
   - All concepts covered, even with different wording â†’ PASS
   - Concepts addressed implicitly when clearly implied â†’ PASS
   - Minor phrasing differences â†’ PASS
   - One or more concepts genuinely absent â†’ FAIL

Response: {response}

Grading Notes: {grading_notes}

Are ALL distinct concepts from the grading notes covered in the response (accepting semantic equivalents and implicit coverage)?""",
    allowed_values=["pass", "fail"],
)
```

!!! tip "ç”¨ LLM ä¼˜åŒ–æç¤ºè¯"
    åœ¨æ¸…æ™°è¯†åˆ«é”™è¯¯æ¨¡å¼åï¼Œå¯ä»¥ç”¨ LLM ä¼˜åŒ–æç¤ºè¯ã€‚ä¹Ÿå¯ä»¥ç”¨ LLM è¾…åŠ©æ‰¾é”™è¯¯ï¼Œä½†éœ€äººå·¥å¤æ ¸ä»¥ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´ã€‚è¿˜å¯ä½¿ç”¨ Cursorã€Claude Code ç­‰ç¼–ç¨‹æ™ºèƒ½ä½“æˆ– [DSPy](https://github.com/stanfordnlp/dspy) ç­‰æ¡†æ¶ç³»ç»Ÿæ€§åœ°ä¼˜åŒ–è¯„åˆ¤æç¤ºè¯ã€‚

## ç”¨æ”¹è¿›åçš„æç¤ºè¯é‡æ–°è¿è¡Œè¯„ä¼°

ç”¨å¢å¼ºçš„ v2 æç¤ºè¯å†è·‘ä¸€éè¯„ä¼°ï¼ˆé…ç½®ä¸åŸºçº¿ç›¸åŒï¼Œä»…æ›¿æ¢æŒ‡æ ‡ï¼‰ï¼š

```python
# Use the same dataset and LLM setup from the baseline evaluation above
results = await judge_experiment.arun(
    dataset,
    name="judge_accuracy_v2_gpt-4o-mini",
    accuracy_metric=accuracy_metric_v2,  # â† Using improved v2 prompt
    llm=llm,
)

passed = sum(1 for r in results if r["alignment"] == "pass")
total = len(results)
print(f"âœ… V2 alignment: {passed}/{total} passed ({passed/total:.1%})")
```

??? "ğŸ“‹ Output (improved v2)"

    ```text
    2025-10-08 23:42:11,650 - Loaded dataset with 160 samples
    2025-10-08 23:42:11,650 - Initializing LLM client with model: gpt-4o-mini
    2025-10-08 23:42:12,730 - Running v2 evaluation with improved prompt...
    Running experiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [04:39<00:00,  1.75s/it]
    2025-10-08 23:46:52,740 - âœ… V2 alignment: 139/160 passed (86.9%)
    ```

**æ˜æ˜¾æå‡ï¼š** å¯¹é½ç‡ä» 75.6% æé«˜åˆ° 86.9%ã€‚

è‹¥éœ€ç»§ç»­è¿­ä»£ï¼š

- åˆ†æå‰©ä½™é”™è¯¯ã€å½’çº³æ¨¡å¼ï¼ˆæ˜¯å‡é˜³æ€§è¿˜æ˜¯å‡é˜´æ€§ï¼Ÿï¼‰
- åœ¨æ ‡ç­¾æ—æ ‡æ³¨ä½ çš„æ¨ç†ï¼Œä¾¿äºæ”¹è¿› LLM è¯„åˆ¤ï¼Œä¹Ÿå¯ä½œä¸º few-shot ç¤ºä¾‹
- **ä½¿ç”¨æ›´å¼ºæ¨¡å‹**ï¼šå¦‚ GPT-5ã€Claude 4.5 Sonnet ç­‰é€šå¸¸ä½œä¸ºè¯„åˆ¤æ›´ç¨³
- **å€ŸåŠ© AI åŠ©æ‰‹**ï¼šæœ¬æŒ‡å—å³ç”¨ Cursor AI åˆ†æå¤±è´¥å¹¶è¿­ä»£æç¤ºè¯ã€‚å¯ä½¿ç”¨ Cursorã€Claude ç­‰ç¼–ç¨‹æ™ºèƒ½ä½“æˆ– [DSPy](https://github.com/stanfordnlp/dspy) ç³»ç»Ÿä¼˜åŒ–è¯„åˆ¤æç¤ºè¯
- å½“å¯¹é½åœ¨ 2â€“3 è½®è¿­ä»£åè¶‹äºç¨³å®šæˆ–è¾¾åˆ°ä¸šåŠ¡é˜ˆå€¼æ—¶å³å¯åœæ­¢

## ä½ å·²å®Œæˆçš„å†…å®¹

ä½ å·²ç”¨ Ragas æ­å»ºäº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„è¯„ä¼°æµæ°´çº¿ï¼Œèƒ½å¤Ÿï¼š

- ç”¨æ¸…æ™°æŒ‡æ ‡è¡¡é‡è¯„åˆ¤ä¸ä¸“å®¶åˆ¤æ–­çš„å¯¹é½åº¦
- é€šè¿‡ç»“æ„åŒ–é”™è¯¯åˆ†æè¯†åˆ«å¤±è´¥æ¨¡å¼
- åœ¨å¯å¤ç°å®éªŒä¸­è·Ÿè¸ªå¤šæ¬¡è¿è¡Œçš„æ”¹è¿›

è¿™ä¸ªå¯¹é½å¥½çš„è¯„åˆ¤å°†æˆä¸ºä½ å¯é  AI è¯„ä¼°çš„åŸºç¡€ã€‚æœ‰äº†å¯ä¿¡çš„è¯„åˆ¤ï¼Œå°±å¯ä»¥æ”¾å¿ƒåœ°è¯„ä¼° RAG æµæ°´çº¿ã€æ™ºèƒ½ä½“å·¥ä½œæµæˆ–ä»»ä½• LLM åº”ç”¨â€”â€”æŒ‡æ ‡æå‡å°†å¯¹åº”çœŸå®çš„è´¨é‡æå‡ã€‚
