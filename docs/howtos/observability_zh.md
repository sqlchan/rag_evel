# å¯è§‚æµ‹æ€§å·¥å…·

## Phoenix (Arize)

### 1. ç®€ä»‹

ä¸º RAG æµæ°´çº¿å»ºç«‹åŸºçº¿é€šå¸¸ä¸éš¾ï¼Œä½†å°†å…¶æå‡åˆ°é€‚åˆç”Ÿäº§å¹¶ä¿è¯å›ç­”è´¨é‡å¾€å¾€å¾ˆéš¾ã€‚åœ¨é€‰é¡¹ä¼—å¤šçš„æƒ…å†µä¸‹ï¼Œä¸º RAG é€‰æ‹©åˆé€‚çš„å·¥å…·å’Œå‚æ•°æœ¬èº«å°±æœ‰æŒ‘æˆ˜ã€‚æœ¬æ•™ç¨‹åˆ†äº«ä¸€å¥—ç¨³å¥çš„å·¥ä½œæµï¼Œå¸®åŠ©ä½ åœ¨æ„å»º RAG æ—¶åšå‡ºæ­£ç¡®é€‰æ‹©å¹¶ä¿è¯è´¨é‡ã€‚

æœ¬æ–‡ä»‹ç»å¦‚ä½•ç»“åˆå¼€æºåº“å¯¹ RAG è¿›è¡Œè¯„ä¼°ã€å¯è§†åŒ–å’Œåˆ†æã€‚æˆ‘ä»¬å°†ä½¿ç”¨ï¼š

- [Ragas](https://docs.ragas.io/en/stable/) è¿›è¡Œåˆæˆæµ‹è¯•æ•°æ®ç”Ÿæˆä¸è¯„ä¼°
- Arize AI çš„ [Phoenix](https://docs.arize.com/phoenix) è¿›è¡Œè¿½è¸ªã€å¯è§†åŒ–å’Œèšç±»åˆ†æ
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/) æ„å»º RAG æµæ°´çº¿

ä¸ºä¾¿äºè¯´æ˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ arXiv ä¸Šå…³äº prompt å·¥ç¨‹çš„è®ºæ–‡æ•°æ®æ¥æ„å»º RAG æµæ°´çº¿ã€‚

â„¹ï¸ æœ¬ notebook éœ€è¦ OpenAI API å¯†é’¥ã€‚

### 2. å®‰è£…ä¾èµ–å¹¶å¯¼å…¥åº“

è¿è¡Œä¸‹æ–¹å•å…ƒæ ¼å®‰è£… Git LFSï¼Œç”¨äºä¸‹è½½æ•°æ®é›†ã€‚


```python
!git lfs install
```

å®‰è£…å¹¶å¯¼å…¥ Python ä¾èµ–ã€‚


```python
!pip install "ragas<0.1.1" pypdf arize-phoenix "openinference-instrumentation-llama-index<1.0.0" "llama-index<0.10.0" pandas
```


```python
import pandas as pd

# Display the complete contents of DataFrame cells.
pd.set_option("display.max_colwidth", None)
```

### 3. é…ç½® OpenAI API å¯†é’¥

è‹¥å°šæœªå°† OpenAI API å¯†é’¥è®¾ä¸ºç¯å¢ƒå˜é‡ï¼Œè¯·è¿›è¡Œè®¾ç½®ã€‚


```python
import os
from getpass import getpass
import openai

if not (openai_api_key := os.getenv("OPENAI_API_KEY")):
    openai_api_key = getpass("ğŸ”‘ Enter your OpenAI API key: ")
openai.api_key = openai_api_key
os.environ["OPENAI_API_KEY"] = openai_api_key
```

### 4. ç”Ÿæˆåˆæˆæµ‹è¯•æ•°æ®é›†

ä¸ºè¯„ä¼°æ•´ç†é»„é‡‘æµ‹è¯•æ•°æ®é›†å¾€å¾€è€—æ—¶ã€ç¹çä¸”æˆæœ¬é«˜ï¼Œå°¤å…¶åœ¨èµ·æ­¥é˜¶æ®µæˆ–æ•°æ®æºé¢‘ç¹å˜åŒ–æ—¶å¹¶ä¸ç°å®ã€‚å¯ä»¥é€šè¿‡åˆæˆç”Ÿæˆé«˜è´¨é‡æ•°æ®ç‚¹å†ç”±å¼€å‘è€…æ ¡éªŒæ¥è§£å†³ï¼Œä»è€Œå°†æ•´ç†æµ‹è¯•æ•°æ®çš„æ—¶é—´ä¸ç²¾åŠ›å‡å°‘çº¦ 90%ã€‚

è¿è¡Œä¸‹æ–¹å•å…ƒæ ¼ä» arXiv ä¸‹è½½ prompt å·¥ç¨‹è®ºæ–‡çš„ PDF æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ LlamaIndex è¯»å–æ–‡æ¡£ã€‚


```python
!git clone https://huggingface.co/datasets/vibrantlabsai/prompt-engineering-papers
```


```python
from llama_index import SimpleDirectoryReader

dir_path = "./prompt-engineering-papers"
reader = SimpleDirectoryReader(dir_path, num_files_limit=2)
documents = reader.load_data()
```

ç†æƒ³çš„æµ‹è¯•æ•°æ®é›†åº”åŒ…å«é«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”ä¸ç”Ÿäº§åˆ†å¸ƒç›¸è¿‘çš„æ•°æ®ç‚¹ã€‚Ragas é‡‡ç”¨ç‹¬ç‰¹çš„åŸºäºè¿›åŒ–çš„åˆæˆæ•°æ®ç”ŸæˆèŒƒå¼ï¼Œç”Ÿæˆé«˜è´¨é‡é—®é¢˜å¹¶ä¿è¯å¤šæ ·æ€§ã€‚Ragas é»˜è®¤ä½¿ç”¨ OpenAI æ¨¡å‹ï¼Œä½ ä¹Ÿå¯ä»¥é€‰ç”¨ä»»æ„æ¨¡å‹ã€‚ä¸‹é¢ä½¿ç”¨ Ragas ç”Ÿæˆ 100 ä¸ªæ•°æ®ç‚¹ã€‚


```python
from ragas.testset import TestsetGenerator
from langchain_openai import ChatOpenAI
from ragas.embeddings import OpenAIEmbeddings
import openai

TEST_SIZE = 25

# generator with openai models
generator_llm = ChatOpenAI(model="gpt-4o-mini")
critic_llm = ChatOpenAI(model="gpt-4o")
openai_client = openai.OpenAI()
embeddings = OpenAIEmbeddings(client=openai_client)

generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)

# generate testset
testset = generator.generate_with_llamaindex_docs(documents, test_size=TEST_SIZE)
test_df = testset.to_pandas()
test_df.head()
```

ä½ å¯ä»¥æŒ‰éœ€è°ƒæ•´é—®é¢˜ç±»å‹åˆ†å¸ƒã€‚æµ‹è¯•é›†å‡†å¤‡å¥½åï¼Œæˆ‘ä»¬ä½¿ç”¨ LlamaIndex æ„å»ºä¸€ä¸ªç®€å•çš„ RAG æµæ°´çº¿ã€‚

### 5. ä½¿ç”¨ LlamaIndex æ„å»º RAG åº”ç”¨

LlamaIndex æ˜¯æ„å»º RAG åº”ç”¨çš„æ˜“ç”¨ã€çµæ´»æ¡†æ¶ã€‚ä¸ºç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„ LLMï¼ˆgpt-3.5-turboï¼‰å’ŒåµŒå…¥æ¨¡å‹ï¼ˆopenai-ada-2ï¼‰ã€‚

åœ¨åå°å¯åŠ¨ Phoenixï¼Œå¹¶å¯¹ LlamaIndex åº”ç”¨è¿›è¡Œæ’æ¡©ï¼Œä½¿ OpenInference span å’Œ trace å‘é€åˆ° Phoenix å¹¶æ±‡æ€»ã€‚[OpenInference](https://github.com/Arize-ai/openinference/tree/main/spec) æ˜¯å»ºç«‹åœ¨ OpenTelemetry ä¹‹ä¸Šçš„å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºæ•è·å’Œå­˜å‚¨ LLM åº”ç”¨æ‰§è¡Œã€‚å®ƒæ—¨åœ¨ä½œä¸ºä¸€ç±»é¥æµ‹æ•°æ®ï¼Œç”¨äºç†è§£ LLM æ‰§è¡ŒåŠå‘¨è¾¹åº”ç”¨ä¸Šä¸‹æ–‡ï¼ˆå¦‚å‘é‡åº“æ£€ç´¢ã€æœç´¢å¼•æ“æˆ– API ç­‰å¤–éƒ¨å·¥å…·çš„ä½¿ç”¨ï¼‰ã€‚


```python
import phoenix as px
from llama_index import set_global_handler

session = px.launch_app()
set_global_handler("arize_phoenix")
```

æ„å»ºæŸ¥è¯¢å¼•æ“ã€‚


```python
from llama_index.core import VectorStoreIndex, ServiceContext
from llama_index.embeddings.openai import OpenAIEmbedding


def build_query_engine(documents):
    vector_index = VectorStoreIndex.from_documents(
        documents,
        service_context=ServiceContext.from_defaults(chunk_size=512),
        embed_model=OpenAIEmbedding(),
    )
    query_engine = vector_index.as_query_engine(similarity_top_k=2)
    return query_engine


query_engine = build_query_engine(documents)
```

åœ¨ Phoenix ä¸­åº”èƒ½çœ‹åˆ°è¯­æ–™ç´¢å¼•æ—¶äº§ç”Ÿçš„ embedding spanã€‚å°†è¿™äº› embedding å¯¼å‡ºå¹¶ä¿å­˜åˆ° DataFrameï¼Œä¾›åç»­å¯è§†åŒ–ä½¿ç”¨ã€‚


```python
from phoenix.trace.dsl import SpanQuery

client = px.Client()
corpus_df = px.Client().query_spans(
    SpanQuery().explode(
        "embedding.embeddings",
        text="embedding.text",
        vector="embedding.vector",
    )
)
corpus_df.head()
```

é‡æ–°å¯åŠ¨ Phoenix ä»¥æ¸…ç©ºå·²ç´¯ç§¯çš„ traceã€‚


```python
px.close_app()
session = px.launch_app()
```

### 6. è¯„ä¼°ä½ çš„ LLM åº”ç”¨

Ragas æä¾›ä¸°å¯Œçš„æŒ‡æ ‡ï¼Œå¯ç”¨äºä»ç»„ä»¶çº§å’Œç«¯åˆ°ç«¯è¯„ä¼° RAG æµæ°´çº¿ã€‚

ä½¿ç”¨ Ragas æ—¶ï¼Œå…ˆæ„é€ è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«é—®é¢˜ã€ç”Ÿæˆç­”æ¡ˆã€æ£€ç´¢ä¸Šä¸‹æ–‡å’Œæ ‡å‡†ç­”æ¡ˆï¼ˆè¯¥é—®é¢˜å¯¹åº”çš„çœŸå®æœŸæœ›ç­”æ¡ˆï¼‰ã€‚


```python
from datasets import Dataset
from tqdm.auto import tqdm
import pandas as pd


def generate_response(query_engine, question):
    response = query_engine.query(question)
    return {
        "answer": response.response,
        "contexts": [c.node.get_content() for c in response.source_nodes],
    }


def generate_ragas_dataset(query_engine, test_df):
    test_questions = test_df["question"].values
    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]

    dataset_dict = {
        "question": test_questions,
        "answer": [response["answer"] for response in responses],
        "contexts": [response["contexts"] for response in responses],
        "ground_truth": test_df["ground_truth"].values.tolist(),
    }
    ds = Dataset.from_dict(dataset_dict)
    return ds


ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)
ragas_evals_df = pd.DataFrame(ragas_eval_dataset)
ragas_evals_df.head()
```

åœ¨ Phoenix ä¸­æŸ¥çœ‹ LlamaIndex åº”ç”¨çš„ traceã€‚


```python
print(session.url)
```

![LlamaIndex åº”ç”¨åœ¨ Phoenix ä¸­çš„ trace](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/ragas/ragas_trace_slide_over.gif)

æˆ‘ä»¬ä¿å­˜ä¸¤ä¸ª DataFrameï¼šä¸€ä¸ªåŒ…å«ç”¨äºåç»­å¯è§†åŒ–çš„ embedding æ•°æ®ï¼Œå¦ä¸€ä¸ªåŒ…å«å¯¼å‡ºçš„ trace å’Œ spanï¼Œä¾› Ragas è¯„ä¼°ä½¿ç”¨ã€‚


```python
# dataset containing embeddings for visualization
query_embeddings_df = px.Client().query_spans(
    SpanQuery().explode(
        "embedding.embeddings", text="embedding.text", vector="embedding.vector"
    )
)
query_embeddings_df.head()
```


```python
from phoenix.session.evaluation import get_qa_with_reference

# dataset containing span data for evaluation with Ragas
spans_dataframe = get_qa_with_reference(client)
spans_dataframe.head()
```

Ragas ä½¿ç”¨ LangChain è¯„ä¼° LLM åº”ç”¨æ•°æ®ã€‚å¯¹ LangChain è¿›è¡Œ OpenInference æ’æ¡©ï¼Œä»¥ä¾¿åœ¨è¯„ä¼° LLM åº”ç”¨æ—¶è§‚å¯Ÿå†…éƒ¨è¿‡ç¨‹ã€‚


```python
from openinference.instrumentation.langchain import LangChainInstrumentor

LangChainInstrumentor().instrument()
```

è¯„ä¼° LLM traceï¼Œå¹¶ä»¥ DataFrame å½¢å¼æŸ¥çœ‹è¯„ä¼°åˆ†æ•°ã€‚


```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_correctness,
    context_recall,
    context_precision,
)

evaluation_result = evaluate(
    dataset=ragas_eval_dataset,
    metrics=[faithfulness, answer_correctness, context_recall, context_precision],
)
eval_scores_df = pd.DataFrame(evaluation_result.scores)
```

å°†è¯„ä¼°ç»“æœæäº¤åˆ° Phoenixï¼Œä½¿å…¶ä½œä¸º span ä¸Šçš„æ ‡æ³¨å¯è§ã€‚


```python
from phoenix.trace import SpanEvaluations

# Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans).
eval_data_df = pd.DataFrame(evaluation_result.dataset)
assert eval_data_df.question.to_list() == list(
    reversed(spans_dataframe.input.to_list())  # The spans are in reverse order.
), "Phoenix spans are in an unexpected order. Re-start the notebook and try again."
eval_scores_df.index = pd.Index(
    list(reversed(spans_dataframe.index.to_list())), name=spans_dataframe.index.name
)

# Log the evaluations to Phoenix.
for eval_name in eval_scores_df.columns:
    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: "score"})
    evals = SpanEvaluations(eval_name, evals_df)
    px.Client().log_evaluations(evals)
```

åœ¨ Phoenix ä¸­å¯ä»¥çœ‹åˆ° Ragas è¯„ä¼°ç»“æœä½œä¸ºåº”ç”¨ span ä¸Šçš„æ ‡æ³¨ã€‚


```python
print(session.url)
```

![Ragas è¯„ä¼°ä½œä¸º span ä¸Šçš„æ ‡æ³¨æ˜¾ç¤º](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/ragas/ragas_evaluation_annotations.gif)

### 7. å¯è§†åŒ–ä¸åˆ†æ Embedding

[Embedding](https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/) ç¼–ç äº†æ£€ç´¢æ–‡æ¡£ä¸ç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰ã€‚å®ƒä»¬ä¸ä»…æ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå¯¹ç†è§£å’Œè°ƒè¯• LLM åº”ç”¨è¡¨ç°ä¹Ÿå¾ˆæœ‰ç”¨ã€‚

Phoenix ä» RAG åº”ç”¨è·å–é«˜ç»´ embeddingï¼Œé™ç»´å¹¶èšç±»ä¸ºæœ‰è¯­ä¹‰çš„æ•°æ®ç»„ã€‚ç„¶åä½ å¯ä»¥é€‰æ‹©æŒ‡æ ‡ï¼ˆå¦‚ Ragas è®¡ç®—çš„ faithfulness æˆ– answer correctnessï¼‰ç›´è§‚æ£€æŸ¥åº”ç”¨è¡¨ç°å¹¶å‘ç°æœ‰é—®é¢˜èšç±»ã€‚è¿™ç§åšæ³•çš„å¥½å¤„æ˜¯åœ¨ç»†ç²’åº¦ä¸”æœ‰æ„ä¹‰çš„æ•°æ®å­é›†ä¸Šæä¾›æŒ‡æ ‡ï¼Œä¾¿äºåˆ†æå±€éƒ¨è¡¨ç°è€Œä¸ä»…æ˜¯å…¨å±€è¡¨ç°ï¼Œä¹Ÿæœ‰åŠ©äºç†è§£åº”ç”¨åœ¨å“ªäº›æŸ¥è¯¢ä¸Šè¡¨ç°ä¸ä½³ã€‚

æˆ‘ä»¬å°†ä»¥ embedding å¯è§†åŒ–æ¨¡å¼é‡æ–°å¯åŠ¨ Phoenixï¼Œåœ¨æµ‹è¯•é›†ä¸Šæ£€æŸ¥åº”ç”¨è¡¨ç°ã€‚


```python
query_embeddings_df = query_embeddings_df.iloc[::-1]
assert ragas_evals_df.question.tolist() == query_embeddings_df.text.tolist()
assert test_df.question.tolist() == ragas_evals_df.question.tolist()
query_df = pd.concat(
    [
        ragas_evals_df[["question", "answer", "ground_truth"]].reset_index(drop=True),
        query_embeddings_df[["vector"]].reset_index(drop=True),
        test_df[["evolution_type"]],
        eval_scores_df.reset_index(drop=True),
    ],
    axis=1,
)
query_df.head()
```


```python
query_schema = px.Schema(
    prompt_column_names=px.EmbeddingColumnNames(
        raw_data_column_name="question", vector_column_name="vector"
    ),
    response_column_names="answer",
)
corpus_schema = px.Schema(
    prompt_column_names=px.EmbeddingColumnNames(
        raw_data_column_name="text", vector_column_name="vector"
    )
)
# relaunch phoenix with a primary and corpus dataset to view embeddings
px.close_app()
session = px.launch_app(
    primary=px.Dataset(query_df, query_schema, "query"),
    corpus=px.Dataset(corpus_df.reset_index(drop=True), corpus_schema, "corpus"),
)
```

å¯åŠ¨ Phoenix åï¼Œå¯æŒ‰ä»¥ä¸‹æ­¥éª¤ç”¨æ‰€é€‰æŒ‡æ ‡å¯è§†åŒ–æ•°æ®ï¼š

- é€‰æ‹© `vector` embeddingï¼Œ
- é€‰æ‹© `Color By > dimension`ï¼Œå†é€‰æ‹©è¦ç€è‰²çš„ç»´åº¦ï¼ˆä¾‹å¦‚æŒ‰ Ragas çš„ faithfulness æˆ– answer correctness ç­‰ç€è‰²ï¼‰ï¼Œ
- åœ¨ `metric` ä¸‹æ‹‰æ¡†ä¸­é€‰æ‹©æŒ‡æ ‡ï¼ŒæŒ‰èšç±»æŸ¥çœ‹èšåˆæŒ‡æ ‡ã€‚

![æ£€æŸ¥ embedding èšç±»ã€æŸ¥çœ‹èšåˆæŒ‡æ ‡ã€æŒ‰æ‰€é€‰æŒ‡æ ‡ç€è‰²](https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/ragas/ragas_correctness_clusters.gif)

### 8. å°ç»“

ä½ å·²ç»ä½¿ç”¨ Ragas å’Œ Phoenix æ„å»ºå¹¶è¯„ä¼°äº† LlamaIndex æŸ¥è¯¢å¼•æ“ã€‚ç®€è¦å›é¡¾å¦‚ä¸‹ï¼š

- ä½¿ç”¨ Ragasï¼šå¼•å¯¼ç”Ÿæˆæµ‹è¯•é›†ï¼Œå¹¶è®¡ç®— faithfulnessã€answer correctness ç­‰æŒ‡æ ‡è¯„ä¼° LlamaIndex æŸ¥è¯¢å¼•æ“ã€‚
- ä½¿ç”¨ OpenInferenceï¼šå¯¹æŸ¥è¯¢å¼•æ“æ’æ¡©ï¼Œè§‚å¯Ÿ LlamaIndex ä¸ Ragas çš„å†…éƒ¨æ‰§è¡Œã€‚
- ä½¿ç”¨ Phoenixï¼šæ”¶é›† span ä¸ traceã€å¯¼å…¥è¯„ä¼°ä¾¿äºæ£€æŸ¥ï¼Œå¹¶å¯è§†åŒ–åµŒå…¥çš„æŸ¥è¯¢ä¸æ£€ç´¢æ–‡æ¡£ä»¥å®šä½è¡¨ç°è¾ƒå·®çš„åŒºåŸŸã€‚

æœ¬ notebook ä»…æ˜¯å¯¹ Ragas ä¸ Phoenix èƒ½åŠ›çš„å…¥é—¨ä»‹ç»ã€‚æ›´å¤šå†…å®¹è¯·å‚é˜… [Ragas](https://docs.ragas.io/en/stable/) ä¸ [Phoenix æ–‡æ¡£](https://docs.arize.com/phoenix/)ã€‚

è‹¥è§‰å¾—æœ¬æ•™ç¨‹æœ‰ç”¨ï¼Œæ¬¢è¿åœ¨ GitHub ç‚¹ â­ï¼š

- [Ragas](https://github.com/vibrantlabsai/ragas)
- [Phoenix](https://github.com/Arize-ai/phoenix)
- [OpenInference](https://github.com/Arize-ai/openinference)

## LangSmith

[LangSmith](https://docs.smith.langchain.com/) æ˜¯ç”¨äºå¢å¼ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„å¼€å‘ä¸éƒ¨ç½²çš„é«˜çº§å·¥å…·ã€‚å®ƒæä¾›è¿½è¸ªã€åˆ†æå’Œä¼˜åŒ– LLM å·¥ä½œæµçš„å®Œæ•´æ¡†æ¶ï¼Œä¾¿äºå¼€å‘è€…ç®¡ç†åº”ç”¨å†…çš„å¤æ‚äº¤äº’ã€‚

æœ¬æ•™ç¨‹è¯´æ˜å¦‚ä½•ä½¿ç”¨ LangSmith è®°å½• Ragas è¯„ä¼°çš„ traceã€‚ç”±äº Ragas åŸºäº LangChain æ„å»ºï¼Œåªéœ€é…ç½® LangSmithï¼Œå³å¯è‡ªåŠ¨è®°å½• traceã€‚

### 1. é…ç½® LangSmith

é…ç½® LangSmith æ—¶ï¼Œè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼ˆè¯¦è§ [LangSmith æ–‡æ¡£](https://docs.smith.langchain.com/#quick-start)ï¼‰ï¼š

```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
export LANGCHAIN_API_KEY=<your-api-key>
export LANGCHAIN_PROJECT=<your-project>  # æœªè®¾ç½®æ—¶é»˜è®¤ä¸º "default"
```

### 2. è·å–æ•°æ®é›†

åˆ›å»ºè¯„ä¼°æ•°æ®é›†æˆ–è¯„ä¼°å®ä¾‹æ—¶ï¼Œè¯·ç¡®ä¿æœ¯è¯­ä¸ `SingleTurnSample` æˆ– `MultiTurnSample` æ‰€ç”¨ schema ä¸€è‡´ã€‚


```python
from ragas import EvaluationDataset


dataset = [
    {
        "user_input": "Which CEO is widely recognized for democratizing AI education through platforms like Coursera?",
        "retrieved_contexts": [
            "Andrew Ng, CEO of Landing AI, is known for his pioneering work in deep learning and for democratizing AI education through Coursera."
        ],
        "response": "Andrew Ng is widely recognized for democratizing AI education through platforms like Coursera.",
        "reference": "Andrew Ng, CEO of Landing AI, is known for his pioneering work in deep learning and for democratizing AI education through Coursera.",
    },
    # ... æ›´å¤šæ ·æœ¬
]

evaluation_dataset = EvaluationDataset.from_list(dataset)
```

### 3. è¿½è¸ª Ragas æŒ‡æ ‡

åœ¨æ•°æ®é›†ä¸Šè¿è¡Œ Ragas è¯„ä¼°åï¼Œtrace ä¼šå‡ºç°åœ¨ LangSmith ä»ªè¡¨ç›˜ä¸­æŒ‡å®šé¡¹ç›®åæˆ– "default" ä¸‹ã€‚


```python
from ragas import evaluate
from ragas.llms import LangchainLLMWrapper
from langchain_openai import ChatOpenAI
from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness

llm = ChatOpenAI(model="gpt-4o-mini")
evaluator_llm = LangchainLLMWrapper(llm)

result = evaluate(
    dataset=evaluation_dataset,
    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],
    llm=evaluator_llm,
)

result
```

è¾“å‡ºç¤ºä¾‹
```
Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]

{'context_recall': 1.0000, 'faithfulness': 0.9333, 'factual_correctness': 0.8520}
```

### 4. LangSmith ä»ªè¡¨ç›˜
![jpeg](./../_static/langsmith_dashboard.png)
