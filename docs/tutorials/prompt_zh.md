# æç¤ºè¯è¯„ä¼°

æœ¬æ•™ç¨‹å°†ç¼–å†™ä¸€ä¸ªç®€å•çš„è¯„ä¼°æµæ°´çº¿ï¼Œç”¨äºè¯„ä¼° AI ç³»ç»Ÿä¸­çš„æç¤ºè¯ï¼Œè¿™é‡Œä»¥ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ç±»å™¨ä¸ºä¾‹ã€‚å­¦å®Œæœ¬æ•™ç¨‹åï¼Œä½ å°†æŒæ¡å¦‚ä½•é€šè¿‡è¯„ä¼°é©±åŠ¨å¼€å‘æ¥è¯„ä¼°å¹¶è¿­ä»£å•ä¸ªæç¤ºè¯ã€‚

```mermaid
flowchart LR
    A["'This movie was amazing!<br/>Great acting and plot.'"] --> B["Classifier Prompt"]
    B --> C["Positive"]
```


æˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€å•æç¤ºè¯å¼€å§‹ï¼šå°†ç”µå½±è¯„è®ºåˆ†ç±»ä¸ºæ­£é¢æˆ–è´Ÿé¢ã€‚

é¦–å…ˆï¼Œè¯·ç¡®ä¿å·²å®‰è£… ragas examples å¹¶é…ç½®å¥½ OpenAI API å¯†é’¥ï¼š

```bash
pip install ragas[examples]
export OPENAI_API_KEY = "your_openai_api_key"
```

ç„¶åæµ‹è¯•è¯¥æç¤ºè¯ï¼š

```bash
python -m ragas_examples.prompt_evals.prompt
```

è¿™å°†ä½¿ç”¨è¾“å…¥ `"The movie was fantastic and I loved every moment of it!"` è¿›è¡Œæµ‹è¯•ï¼Œè¾“å‡ºåº”ä¸º `"positive"`ã€‚

> **ğŸ’¡ å¿«é€Ÿå¼€å§‹**ï¼šè‹¥æƒ³ç›´æ¥çœ‹åˆ°å®Œæ•´è¯„ä¼°æ•ˆæœï¼Œå¯è·³è½¬åˆ°[ç«¯åˆ°ç«¯è¿è¡Œç¤ºä¾‹](#ç«¯åˆ°ç«¯è¿è¡Œç¤ºä¾‹)ï¼Œä¸€é”®è¿è¡Œå¹¶è‡ªåŠ¨ç”Ÿæˆ CSV ç»“æœã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºæç¤ºè¯å†™ä¸‹è‹¥å¹²ç¤ºä¾‹è¾“å…¥å’ŒæœŸæœ›è¾“å‡ºï¼Œå¹¶è½¬æ¢ä¸º CSV æ–‡ä»¶ã€‚

```python
import pandas as pd

samples = [{"text": "I loved the movie! It was fantastic.", "label": "positive"},
    {"text": "The movie was terrible and boring.", "label": "negative"},
    {"text": "It was an average film, nothing special.", "label": "positive"},
    {"text": "Absolutely amazing! Best movie of the year.", "label": "positive"}]
pd.DataFrame(samples).to_csv("datasets/test_dataset.csv", index=False)
```

æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹å¼è¡¡é‡æç¤ºè¯åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªæŒ‡æ ‡ï¼Œå°†æç¤ºè¯è¾“å‡ºä¸æœŸæœ›è¾“å‡ºæ¯”è¾ƒï¼Œå¹¶æ®æ­¤è¾“å‡ºé€šè¿‡/ä¸é€šè¿‡ã€‚

```python
from ragas.metrics import discrete_metric
from ragas.metrics.result import MetricResult

@discrete_metric(name="accuracy", allowed_values=["pass", "fail"])
def my_metric(prediction: str, actual: str):
    """Calculate accuracy of the prediction."""
    return MetricResult(value="pass", reason="") if prediction == actual else MetricResult(value="fail", reason="")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç¼–å†™å®éªŒå¾ªç¯ï¼šåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¿è¡Œæç¤ºè¯ï¼Œä½¿ç”¨è¯¥æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° CSV æ–‡ä»¶ã€‚

```python
from ragas import experiment

@experiment()
async def run_experiment(row):
    
    response = run_prompt(row["text"])
    score = my_metric.score(
        prediction=response,
        actual=row["label"]
    )

    experiment_view = {
        **row,
        "response":response,
        "score":score.value,
    }
    return experiment_view
```

æ­¤åï¼Œæ¯æ¬¡ä¿®æ”¹æç¤ºè¯æ—¶ï¼Œéƒ½å¯ä»¥è¿è¡Œå®éªŒï¼ŒæŸ¥çœ‹å¯¹æç¤ºè¯æ€§èƒ½çš„å½±å“ã€‚

### ä¼ é€’é¢å¤–å‚æ•°

ä½ å¯ä»¥å‘å®éªŒå‡½æ•°ä¼ é€’é¢å¤–å‚æ•°ï¼Œä¾‹å¦‚æ¨¡å‹æˆ–é…ç½®ï¼š

```python
@experiment()
async def run_experiment(row, model):
    response = run_prompt(row["text"], model=model)
    score = my_metric.score(
        prediction=response,
        actual=row["label"]
    )

    experiment_view = {
        **row,
        "response": response,
        "score": score.value,
    }
    return experiment_view

# Run with specific parameters
run_experiment.arun(dataset, "gpt-4")

# Or use keyword arguments
run_experiment.arun(dataset, model="gpt-4o")
```


## ç«¯åˆ°ç«¯è¿è¡Œç¤ºä¾‹

1. é…ç½®ä½ çš„ OpenAI API å¯†é’¥
```bash
export OPENAI_API_KEY = "your_openai_api_key"
```
2. è¿è¡Œè¯„ä¼°
```bash
python -m ragas_examples.prompt_evals.evals
```

è¯¥å‘½ä»¤å°†ï¼š

- åˆ›å»ºåŒ…å«ç¤ºä¾‹ç”µå½±è¯„è®ºçš„æµ‹è¯•æ•°æ®é›†
- å¯¹æ¯ä¸ªæ ·æœ¬è¿è¡Œæƒ…æ„Ÿåˆ†ç±»æç¤ºè¯
- ä½¿ç”¨å‡†ç¡®ç‡æŒ‡æ ‡è¯„ä¼°ç»“æœ
- å°†æ‰€æœ‰ç»“æœå¯¼å‡ºåˆ° CSV æ–‡ä»¶

å®Œæˆï¼ä½ å·²ç»æˆåŠŸä½¿ç”¨ Ragas å®Œæˆäº†ç¬¬ä¸€æ¬¡è¯„ä¼°ã€‚ç°åœ¨å¯ä»¥æ‰“å¼€ `experiments/experiment_name.csv` æ–‡ä»¶æŸ¥çœ‹ç»“æœã€‚
